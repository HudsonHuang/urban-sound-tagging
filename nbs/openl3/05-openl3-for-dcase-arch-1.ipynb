{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from RandomErasing import RandomErasing\n",
    "\n",
    "import torchvision.models\n",
    "from torchvision import transforms\n",
    "\n",
    "from albumentations import Compose, ShiftScaleRotate, GridDistortion\n",
    "from albumentations.pytorch import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, unknown_to_known):\n",
    "    df = df.reset_index()\n",
    "    df['slno'] = df.assign(slno=1).groupby('audio_filename')['slno'].cumsum()\n",
    "    df.set_index(['audio_filename', 'slno'], inplace=True)\n",
    "\n",
    "    df_unknown = df.copy().loc[:, list(unknown_to_known.keys())]\n",
    "    df.drop(columns=list(unknown_to_known.keys()), inplace=True)\n",
    "\n",
    "    y_mask = df.copy()\n",
    "    y_mask.loc[:, :] = 1\n",
    "    for unknown, known in unknown_to_known.items():\n",
    "        y_mask.loc[\n",
    "            df_unknown[unknown] > 0.5,\n",
    "            known\n",
    "        ] = 0\n",
    "\n",
    "    df = df.swaplevel(i=1, j=0, axis=0).sort_index()\n",
    "\n",
    "    y_mask = y_mask.swaplevel(i=1, j=0, axis=0).sort_index()\n",
    "\n",
    "    y = np.concatenate([\n",
    "        df.loc[[1], :].values[..., np.newaxis],\n",
    "        df.loc[[2], :].values[..., np.newaxis],\n",
    "        df.loc[[3], :].values[..., np.newaxis]\n",
    "    ], axis=2)\n",
    "\n",
    "    y_mask = np.concatenate([\n",
    "        y_mask.loc[[1], :].values[..., np.newaxis],\n",
    "        y_mask.loc[[2], :].values[..., np.newaxis],\n",
    "        y_mask.loc[[3], :].values[..., np.newaxis]\n",
    "    ], axis=2)\n",
    "\n",
    "    X = np.concatenate([\n",
    "        np.expand_dims(np.load('../../data/logmelspec/{}.npy'.format(x)).T[:635, :], axis=0)\n",
    "        for x in df.loc[[1], :].reset_index(1).audio_filename.tolist()])\n",
    "    X = np.expand_dims(X, axis=1)\n",
    "\n",
    "    return X, y, y_mask\n",
    "\n",
    "\n",
    "random_erasing = RandomErasing()\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, weights, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights\n",
    "        self.transform = transform\n",
    "        self.pil = transforms.ToPILImage()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx, ...]\n",
    "\n",
    "        if self.transform:\n",
    "            # min-max transformation\n",
    "            this_min = sample.min()\n",
    "            this_max = sample.max()\n",
    "            sample = (sample - this_min) / (this_max - this_min)\n",
    "\n",
    "            # randomly cycle the file\n",
    "            i = np.random.randint(sample.shape[1])\n",
    "            sample = torch.cat([\n",
    "                sample[:, i:, :],\n",
    "                sample[:, :i, :]],\n",
    "                dim=1)\n",
    "\n",
    "            # apply albumentations transforms\n",
    "            sample = np.array(self.pil(sample))\n",
    "            sample = self.transform(image=sample)\n",
    "            sample = sample['image']\n",
    "            sample = sample[None, :, :].permute(0, 2, 1)\n",
    "\n",
    "            # apply random erasing\n",
    "            sample = random_erasing(sample.clone().detach())\n",
    "\n",
    "            # revert min-max transformation\n",
    "            sample = (sample * (this_max - this_min)) + this_min\n",
    "\n",
    "        return sample, self.y[idx, ...], self.weights[idx, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "with open('../../data/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "unknown_to_known = (\n",
    "    pd.merge(metadata['taxonomy_df'].loc[lambda x: x.fine_id == 'X', ['fine', 'coarse']],\n",
    "             metadata['taxonomy_df'].loc[lambda x: x.fine_id != 'X', ['fine', 'coarse']],\n",
    "             on='coarse', how='inner')\n",
    "    .drop(columns='coarse')\n",
    "    .groupby('fine_x')['fine_y']\n",
    "    .apply(lambda x: list(x)).to_dict())\n",
    "known_labels = metadata['taxonomy_df'].loc[lambda x: x.fine_id != 'X'].fine.tolist()\n",
    "\n",
    "train_df = pd.concat([metadata['coarse_train'], metadata['fine_train']], axis=1, sort=True)\n",
    "valid_df = pd.concat([metadata['coarse_test'], metadata['fine_test']], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual correction for one data point\n",
    "train_df.loc[(train_df.sum(axis=1) == 37).copy(), :] = 0\n",
    "valid_df.loc[(valid_df.sum(axis=1) == 37).copy(), :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, train_y_mask = prepare_data(train_df, unknown_to_known)\n",
    "valid_X, valid_y, valid_y_mask = prepare_data(valid_df, unknown_to_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise normalization\n",
    "channel_means = train_X.reshape(-1, 128).mean(axis=0).reshape(1, 1, 1, -1)\n",
    "channel_stds = train_X.reshape(-1, 128).std(axis=0).reshape(1, 1, 1, -1)\n",
    "train_X = (train_X - channel_means) / channel_stds\n",
    "valid_X = (valid_X - channel_means) / channel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data augmentation transformations\n",
    "albumentations_transform = Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0.5),\n",
    "    GridDistortion(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and the dataloaders\n",
    "train_dataset = AudioDataset(torch.Tensor(train_X),\n",
    "                             torch.Tensor(train_y),\n",
    "                             torch.Tensor(train_y_mask),\n",
    "                             albumentations_transform)\n",
    "valid_dataset = AudioDataset(torch.Tensor(valid_X),\n",
    "                             torch.Tensor(valid_y),\n",
    "                             torch.Tensor(valid_y_mask),\n",
    "                             None)\n",
    "\n",
    "val_loader = DataLoader(valid_dataset, 16, shuffle=False)\n",
    "train_loader_1 = DataLoader(train_dataset, 16, shuffle=True)\n",
    "train_loader_2 = DataLoader(train_dataset, 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.320325501025387,\n",
       " -1.6221432905186037e-13,\n",
       " -0.20615205257469454,\n",
       " 5.856143103520437,\n",
       " -0.7684339386533267,\n",
       " 0.6658041000484232)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.min(train_X),\n",
    " np.mean(train_X),\n",
    " np.median(train_X),\n",
    " np.max(train_X),\n",
    " np.quantile(train_X, 0.25),\n",
    " np.quantile(train_X, 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(train_X[0].reshape(-1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.220554099317004,\n",
       " -0.04937556515118184,\n",
       " -0.2779255644429254,\n",
       " 6.532760464119584,\n",
       " -0.6986408119781202,\n",
       " 0.40127430650001294)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.min(valid_X),\n",
    " np.mean(valid_X),\n",
    " np.median(valid_X),\n",
    " np.max(valid_X),\n",
    " np.quantile(valid_X, 0.25),\n",
    " np.quantile(valid_X, 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(valid_X[0].reshape(-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Define the device to be used\n",
    "cuda = True\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(weight_file):\n",
    "    if weight_file == None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        weights_dict = np.load(weight_file).item()\n",
    "    except:\n",
    "        weights_dict = np.load(weight_file, encoding='bytes').item()\n",
    "\n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KitModel(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, weight_file):\n",
    "        super(KitModel, self).__init__()\n",
    "        self.__weights_dict = load_weights(weight_file)\n",
    "\n",
    "        self.batch_normalization_1 = self.__batch_normalization(2, 'batch_normalization_1', num_features=1, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_1 = self.__conv(2, name='conv2d_1', in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_2 = self.__batch_normalization(2, 'batch_normalization_2', num_features=64, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_2 = self.__conv(2, name='conv2d_2', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_3 = self.__batch_normalization(2, 'batch_normalization_3', num_features=64, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_3 = self.__conv(2, name='conv2d_3', in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_4 = self.__batch_normalization(2, 'batch_normalization_4', num_features=128, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_4 = self.__conv(2, name='conv2d_4', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_5 = self.__batch_normalization(2, 'batch_normalization_5', num_features=128, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_5 = self.__conv(2, name='conv2d_5', in_channels=128, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_6 = self.__batch_normalization(2, 'batch_normalization_6', num_features=256, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_6 = self.__conv(2, name='conv2d_6', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_7 = self.__batch_normalization(2, 'batch_normalization_7', num_features=256, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_7 = self.__conv(2, name='conv2d_7', in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_8 = self.__batch_normalization(2, 'batch_normalization_8', num_features=512, eps=0.001, momentum=0.99)\n",
    "        self.audio_embedding_layer = self.__conv(2, name='audio_embedding_layer', in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv2d_1_pad    = F.pad(x, (1, 1, 1, 1))\n",
    "        conv2d_1        = self.conv2d_1(conv2d_1_pad)\n",
    "        batch_normalization_2 = self.batch_normalization_2(conv2d_1)\n",
    "        activation_1    = F.relu(batch_normalization_2)\n",
    "        conv2d_2_pad    = F.pad(activation_1, (1, 1, 1, 1))\n",
    "        conv2d_2        = self.conv2d_2(conv2d_2_pad)\n",
    "        batch_normalization_3 = self.batch_normalization_3(conv2d_2)\n",
    "        activation_2    = F.relu(batch_normalization_3)\n",
    "        max_pooling2d_1 = F.max_pool2d(activation_2, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n",
    "        conv2d_3_pad    = F.pad(max_pooling2d_1, (1, 1, 1, 1))\n",
    "        conv2d_3        = self.conv2d_3(conv2d_3_pad)\n",
    "        batch_normalization_4 = self.batch_normalization_4(conv2d_3)\n",
    "        activation_3    = F.relu(batch_normalization_4)\n",
    "        conv2d_4_pad    = F.pad(activation_3, (1, 1, 1, 1))\n",
    "        conv2d_4        = self.conv2d_4(conv2d_4_pad)\n",
    "        batch_normalization_5 = self.batch_normalization_5(conv2d_4)\n",
    "        activation_4    = F.relu(batch_normalization_5)\n",
    "        max_pooling2d_2 = F.max_pool2d(activation_4, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n",
    "        conv2d_5_pad    = F.pad(max_pooling2d_2, (1, 1, 1, 1))\n",
    "        conv2d_5        = self.conv2d_5(conv2d_5_pad)\n",
    "        batch_normalization_6 = self.batch_normalization_6(conv2d_5)\n",
    "        activation_5    = F.relu(batch_normalization_6)\n",
    "        conv2d_6_pad    = F.pad(activation_5, (1, 1, 1, 1))\n",
    "        conv2d_6        = self.conv2d_6(conv2d_6_pad)\n",
    "        batch_normalization_7 = self.batch_normalization_7(conv2d_6)\n",
    "        activation_6    = F.relu(batch_normalization_7)\n",
    "        max_pooling2d_3 = F.max_pool2d(activation_6, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n",
    "        conv2d_7_pad    = F.pad(max_pooling2d_3, (1, 1, 1, 1))\n",
    "        conv2d_7        = self.conv2d_7(conv2d_7_pad)\n",
    "        batch_normalization_8 = self.batch_normalization_8(conv2d_7)\n",
    "        activation_7    = F.relu(batch_normalization_8)\n",
    "        audio_embedding_layer_pad = F.pad(activation_7, (1, 1, 1, 1))\n",
    "        audio_embedding_layer = self.audio_embedding_layer(audio_embedding_layer_pad)\n",
    "        max_pooling2d_4 = F.max_pool2d(audio_embedding_layer, kernel_size=(4, 8), stride=(4, 8), padding=0, ceil_mode=False)\n",
    "        return max_pooling2d_4\n",
    "\n",
    "\n",
    "    def __batch_normalization(self, dim, name, **kwargs):\n",
    "        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)\n",
    "        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)\n",
    "        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)\n",
    "        else:           raise NotImplementedError()\n",
    "\n",
    "        if 'scale' in self.__weights_dict[name]:\n",
    "            layer.state_dict()['weight'].copy_(torch.from_numpy(self.__weights_dict[name]['scale']))\n",
    "        else:\n",
    "            layer.weight.data.fill_(1)\n",
    "\n",
    "        if 'bias' in self.__weights_dict[name]:\n",
    "            layer.state_dict()['bias'].copy_(torch.from_numpy(self.__weights_dict[name]['bias']))\n",
    "        else:\n",
    "            layer.bias.data.fill_(0)\n",
    "\n",
    "        layer.state_dict()['running_mean'].copy_(torch.from_numpy(self.__weights_dict[name]['mean']))\n",
    "        layer.state_dict()['running_var'].copy_(torch.from_numpy(self.__weights_dict[name]['var']))\n",
    "        return layer\n",
    "\n",
    "    def __conv(self, dim, name, **kwargs):\n",
    "        if   dim == 1:  layer = nn.Conv1d(**kwargs)\n",
    "        elif dim == 2:  layer = nn.Conv2d(**kwargs)\n",
    "        elif dim == 3:  layer = nn.Conv3d(**kwargs)\n",
    "        else:           raise NotImplementedError()\n",
    "\n",
    "        layer.state_dict()['weight'].copy_(torch.from_numpy(self.__weights_dict[name]['weights']))\n",
    "        if 'bias' in self.__weights_dict[name]:\n",
    "            layer.state_dict()['bias'].copy_(torch.from_numpy(self.__weights_dict[name]['bias']))\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task5Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        self.openl3 = KitModel('./openl3_no_mel_layer_pytorch_weights')\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 1, 3, 2) # keras model had (92, 128, 199, 1) like shape\n",
    "        x = self.bn(x)\n",
    "        x = self.openl3(x)\n",
    "        x = x.max(dim=-1)[0].max(dim=-1)[0]\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Task5Model(31).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task5Model(\n",
       "  (bn): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (openl3): KitModel(\n",
       "    (batch_normalization_1): BatchNorm2d(1, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (conv2d_1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batch_normalization_2): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (conv2d_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batch_normalization_3): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (conv2d_3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batch_normalization_4): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (conv2d_4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batch_normalization_5): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (conv2d_5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batch_normalization_6): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (conv2d_6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batch_normalization_7): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (conv2d_7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (batch_normalization_8): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "    (audio_embedding_layer): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1))\n",
       "  )\n",
       "  (final): Sequential(\n",
       "    (0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Linear(in_features=512, out_features=31, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer, scheduler and loss criteria\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, amsgrad=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "0.4053396038254913 0.1620012702686446\n",
      "Epoch:  1\n",
      "0.17613646266411762 0.15798584584678924\n",
      "Epoch:  2\n",
      "0.1733832455613986 0.17738483580095427\n",
      "Epoch:  3\n",
      "0.17020063173203243 0.15283765643835068\n",
      "Epoch:  4\n",
      "0.16867618658104722 0.176861756082092\n",
      "Epoch:  5\n",
      "0.16630145167412402 0.15322211597646987\n",
      "Epoch:  6\n",
      "0.16524767845260854 0.1593046643372093\n",
      "Epoch:  7\n",
      "0.1646308768160489 0.14973355137876102\n",
      "Epoch:  8\n",
      "0.16359720627466837 0.14084659383765288\n",
      "Epoch:  9\n",
      "0.16243001119214662 0.16445187026900904\n",
      "Epoch:  10\n",
      "0.16126727267187468 0.13925366795488767\n",
      "Epoch:  11\n",
      "0.161516390183345 0.13796378139938628\n",
      "Epoch:  12\n",
      "0.16044095330903319 0.13887232887957776\n",
      "Epoch:  13\n",
      "0.15957330187567237 0.13806300849786826\n",
      "Epoch:  14\n",
      "0.1590770064365296 0.13792910453464305\n",
      "Epoch:  15\n",
      "0.15837847360340104 0.13701242288308485\n",
      "Epoch:  16\n",
      "0.1584162662933473 0.1352191317294325\n",
      "Epoch:  17\n",
      "0.1579636491480328 0.13493077669824874\n",
      "Epoch:  18\n",
      "0.15815928117150352 0.14395052487296717\n",
      "Epoch:  19\n",
      "0.15781083358388368 0.14023449192089693\n",
      "Epoch:  20\n",
      "0.1581280905069137 0.13540356739291123\n",
      "Epoch:  21\n",
      "0.1571126652209937 0.13989842762904509\n",
      "Epoch:  22\n",
      "0.15620902520255978 0.13268291684133665\n",
      "Epoch:  23\n",
      "0.15582944067562518 0.134210112371615\n",
      "Epoch:  24\n",
      "0.15561067120355815 0.13366718350776605\n",
      "Epoch:  25\n",
      "0.1552592314222232 0.13745359224932535\n",
      "Epoch:  26\n",
      "0.1547864004969597 0.13584489109260695\n",
      "Epoch:  27\n",
      "0.15440281592056054 0.13063252743865764\n",
      "Epoch:  28\n",
      "0.1545977595002473 0.13171578517981938\n",
      "Epoch:  29\n",
      "0.15314964950084686 0.13074003638965742\n",
      "Epoch:  30\n",
      "0.1535767759291493 0.13114901099886214\n",
      "Epoch:  31\n",
      "0.1539506834684586 0.13362992155764783\n",
      "Epoch:  32\n",
      "0.15249148305176066 0.13820253791553633\n",
      "Epoch:  33\n",
      "0.15374484521393872 0.13383058325520583\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:  34\n",
      "0.15070507553767185 0.12800615121211326\n",
      "Epoch:  35\n",
      "0.1502077961657323 0.1271533968725375\n",
      "Epoch:  36\n",
      "0.1513585050393935 0.12719522948775971\n",
      "Epoch:  37\n",
      "0.15023000239312242 0.1293873337230512\n",
      "Epoch:  38\n",
      "0.14966001635303303 0.12852094694972038\n",
      "Epoch:  39\n",
      "0.1490794144013301 0.12724381791693823\n",
      "Epoch:  40\n",
      "0.15030787297252085 0.12589840910264424\n",
      "Epoch:  41\n",
      "0.15022708673258217 0.12624363414943218\n",
      "Epoch:  42\n",
      "0.14928690288342586 0.12568885566932814\n",
      "Epoch:  43\n",
      "0.14925837572537312 0.1281599077795233\n",
      "Epoch:  44\n",
      "0.1482767085639798 0.12661546814654553\n",
      "Epoch:  45\n",
      "0.14771838617973587 0.12701137975922652\n",
      "Epoch:  46\n",
      "0.1485729905314186 0.1256018350166934\n",
      "Epoch:  47\n",
      "0.14797572887876406 0.12601278509412492\n",
      "Epoch:  48\n",
      "0.14957582043344472 0.12545670568943024\n",
      "Epoch:  49\n",
      "0.14895700024706976 0.1262547615915537\n",
      "Epoch:  50\n",
      "0.1483067451488404 0.12569291730012214\n",
      "Epoch:  51\n",
      "0.14825134379725877 0.12711144531411783\n",
      "Epoch:  52\n",
      "0.1492533286919399 0.12622294069400855\n",
      "Epoch:  53\n",
      "0.14860675098741946 0.12588851100632123\n",
      "Epoch:  54\n",
      "0.14795674708019307 0.12547034344502858\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:  55\n",
      "0.1487349628084371 0.1265241831008877\n",
      "Epoch:  56\n",
      "0.14828633476479525 0.1258296985179186\n",
      "Epoch:  57\n",
      "0.14797574236076705 0.12552008884293692\n",
      "Epoch:  58\n",
      "0.1477767925481407 0.12740840683025972\n",
      "Epoch:  59\n",
      "0.1470693240360338 0.1257052214017936\n",
      "Epoch:  60\n",
      "0.14728989619381574 0.12740628022168363\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:  61\n",
      "0.1484797390646675 0.12527538197381155\n",
      "Epoch:  62\n",
      "0.1479850208678213 0.12468938822192806\n",
      "Epoch:  63\n",
      "0.14791792621012448 0.12475011045379299\n",
      "Epoch:  64\n",
      "0.1479018273300865 0.12549734088991368\n",
      "Epoch:  65\n",
      "0.14855758161569127 0.12737574907285826\n",
      "Epoch:  66\n",
      "0.14710691013709218 0.1248813375298466\n",
      "Epoch:  67\n",
      "0.1485282431046168 0.12675455106156214\n",
      "Epoch:  68\n",
      "0.14772212895609083 0.1263512982321637\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:  69\n",
      "0.14810349006636614 0.12556109657245024\n",
      "Epoch:  70\n",
      "0.14879175681037968 0.12681276510868753\n",
      "Epoch:  71\n",
      "0.14906812819088397 0.12667158458914077\n",
      "Epoch:  72\n",
      "0.14775930213279465 0.1257861703634262\n",
      "Epoch:  73\n",
      "0.14863486760327604 0.12483815662562847\n",
      "Epoch:  74\n",
      "0.14942149103296046 0.12648234064025538\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:  75\n",
      "0.14847059137358956 0.1264004231031452\n",
      "Epoch:  76\n",
      "0.14933425360390928 0.12580329099936144\n",
      "Epoch:  77\n",
      "0.14872634466610798 0.1246668097696134\n",
      "Epoch:  78\n",
      "0.1492362476023687 0.12488033063709736\n",
      "Epoch:  79\n",
      "0.14779516922778824 0.1264157383037465\n",
      "Epoch:  80\n",
      "0.14782849121458677 0.1263839229941368\n",
      "Epoch:  81\n",
      "0.14820421532708772 0.126785154321364\n",
      "Epoch:  82\n",
      "0.14812090415127424 0.12507155298122338\n",
      "Epoch:  83\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "train_loss_hist = []\n",
    "valid_loss_hist = []\n",
    "lowest_val_loss = np.inf\n",
    "epochs_without_new_lowest = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch: ', i)\n",
    "\n",
    "    this_epoch_train_loss = 0\n",
    "    for i1, i2 in zip(train_loader_1, train_loader_2):\n",
    "\n",
    "        # mixup the inputs ---------\n",
    "        alpha = 1\n",
    "        mixup_vals = np.random.beta(alpha, alpha, i1[0].shape[0])\n",
    "\n",
    "        lam = torch.Tensor(mixup_vals.reshape(mixup_vals.shape[0], 1, 1, 1))\n",
    "        inputs = (lam * i1[0]) + ((1 - lam) * i2[0])\n",
    "\n",
    "        lam = torch.Tensor(mixup_vals.reshape(mixup_vals.shape[0], 1, 1))\n",
    "        labels = (lam * i1[1]) + ((1 - lam) * i2[1])\n",
    "        masks = (lam * i1[2]) + ((1 - lam) * i2[2])\n",
    "        # mixup ends ----------\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            model = model.train()\n",
    "            outputs = model(inputs)\n",
    "            # calculate loss for each set of annotations\n",
    "            loss_0 = criterion(outputs, labels[:, :, 0]) * masks[:, :, 0]\n",
    "            loss_1 = criterion(outputs, labels[:, :, 1]) * masks[:, :, 1]\n",
    "            loss_2 = criterion(outputs, labels[:, :, 2]) * masks[:, :, 2]\n",
    "            loss = (loss_0.sum() + loss_1.sum() + loss_2.sum()) / masks.sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            this_epoch_train_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "    this_epoch_valid_loss = 0\n",
    "    for inputs, labels, masks in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            model = model.eval()\n",
    "            outputs = model(inputs)\n",
    "            loss_0 = criterion(outputs, labels[:, :, 0]) * masks[:, :, 0]\n",
    "            loss_1 = criterion(outputs, labels[:, :, 1]) * masks[:, :, 1]\n",
    "            loss_2 = criterion(outputs, labels[:, :, 2]) * masks[:, :, 2]\n",
    "            loss = (loss_0.sum() + loss_1.sum() + loss_2.sum()) / masks.sum()\n",
    "            this_epoch_valid_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "    this_epoch_train_loss /= len(train_loader_1)\n",
    "    this_epoch_valid_loss /= len(val_loader)\n",
    "\n",
    "    train_loss_hist.append(this_epoch_train_loss)\n",
    "    valid_loss_hist.append(this_epoch_valid_loss)\n",
    "\n",
    "    if this_epoch_valid_loss < lowest_val_loss:\n",
    "        lowest_val_loss = this_epoch_valid_loss\n",
    "        torch.save(model.state_dict(), './model_system1')\n",
    "        epochs_without_new_lowest = 0\n",
    "    else:\n",
    "        epochs_without_new_lowest += 1\n",
    "\n",
    "    if epochs_without_new_lowest >= 25:\n",
    "        break\n",
    "\n",
    "    print(this_epoch_train_loss, this_epoch_valid_loss)\n",
    "\n",
    "    scheduler.step(this_epoch_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
