{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/io/_video_opt.py:17: UserWarning: video reader based on ffmpeg c++ ops not available\n",
      "  warnings.warn(\"video reader based on ffmpeg c++ ops not available\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(weight_file):\n",
    "    if weight_file == None:\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        weights_dict = np.load(weight_file).item()\n",
    "    except:\n",
    "        weights_dict = np.load(weight_file, encoding='bytes').item()\n",
    "\n",
    "    return weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KitModel(nn.Module):\n",
    "\n",
    "    \n",
    "    def __init__(self, weight_file):\n",
    "        super(KitModel, self).__init__()\n",
    "        self.__weights_dict = load_weights(weight_file)\n",
    "\n",
    "        self.batch_normalization_1 = self.__batch_normalization(2, 'batch_normalization_1', num_features=1, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_1 = self.__conv(2, name='conv2d_1', in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_2 = self.__batch_normalization(2, 'batch_normalization_2', num_features=64, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_2 = self.__conv(2, name='conv2d_2', in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_3 = self.__batch_normalization(2, 'batch_normalization_3', num_features=64, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_3 = self.__conv(2, name='conv2d_3', in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_4 = self.__batch_normalization(2, 'batch_normalization_4', num_features=128, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_4 = self.__conv(2, name='conv2d_4', in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_5 = self.__batch_normalization(2, 'batch_normalization_5', num_features=128, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_5 = self.__conv(2, name='conv2d_5', in_channels=128, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_6 = self.__batch_normalization(2, 'batch_normalization_6', num_features=256, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_6 = self.__conv(2, name='conv2d_6', in_channels=256, out_channels=256, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_7 = self.__batch_normalization(2, 'batch_normalization_7', num_features=256, eps=0.001, momentum=0.99)\n",
    "        self.conv2d_7 = self.__conv(2, name='conv2d_7', in_channels=256, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "        self.batch_normalization_8 = self.__batch_normalization(2, 'batch_normalization_8', num_features=512, eps=0.001, momentum=0.99)\n",
    "        self.audio_embedding_layer = self.__conv(2, name='audio_embedding_layer', in_channels=512, out_channels=512, kernel_size=(3, 3), stride=(1, 1), groups=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv2d_1_pad    = F.pad(x, (1, 1, 1, 1))\n",
    "        conv2d_1        = self.conv2d_1(conv2d_1_pad)\n",
    "        batch_normalization_2 = self.batch_normalization_2(conv2d_1)\n",
    "        activation_1    = F.relu(batch_normalization_2)\n",
    "        conv2d_2_pad    = F.pad(activation_1, (1, 1, 1, 1))\n",
    "        conv2d_2        = self.conv2d_2(conv2d_2_pad)\n",
    "        batch_normalization_3 = self.batch_normalization_3(conv2d_2)\n",
    "        activation_2    = F.relu(batch_normalization_3)\n",
    "        max_pooling2d_1 = F.max_pool2d(activation_2, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n",
    "        conv2d_3_pad    = F.pad(max_pooling2d_1, (1, 1, 1, 1))\n",
    "        conv2d_3        = self.conv2d_3(conv2d_3_pad)\n",
    "        batch_normalization_4 = self.batch_normalization_4(conv2d_3)\n",
    "        activation_3    = F.relu(batch_normalization_4)\n",
    "        conv2d_4_pad    = F.pad(activation_3, (1, 1, 1, 1))\n",
    "        conv2d_4        = self.conv2d_4(conv2d_4_pad)\n",
    "        batch_normalization_5 = self.batch_normalization_5(conv2d_4)\n",
    "        activation_4    = F.relu(batch_normalization_5)\n",
    "        max_pooling2d_2 = F.max_pool2d(activation_4, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n",
    "        conv2d_5_pad    = F.pad(max_pooling2d_2, (1, 1, 1, 1))\n",
    "        conv2d_5        = self.conv2d_5(conv2d_5_pad)\n",
    "        batch_normalization_6 = self.batch_normalization_6(conv2d_5)\n",
    "        activation_5    = F.relu(batch_normalization_6)\n",
    "        conv2d_6_pad    = F.pad(activation_5, (1, 1, 1, 1))\n",
    "        conv2d_6        = self.conv2d_6(conv2d_6_pad)\n",
    "        batch_normalization_7 = self.batch_normalization_7(conv2d_6)\n",
    "        activation_6    = F.relu(batch_normalization_7)\n",
    "        max_pooling2d_3 = F.max_pool2d(activation_6, kernel_size=(2, 2), stride=(2, 2), padding=0, ceil_mode=False)\n",
    "        conv2d_7_pad    = F.pad(max_pooling2d_3, (1, 1, 1, 1))\n",
    "        conv2d_7        = self.conv2d_7(conv2d_7_pad)\n",
    "        batch_normalization_8 = self.batch_normalization_8(conv2d_7)\n",
    "        activation_7    = F.relu(batch_normalization_8)\n",
    "        audio_embedding_layer_pad = F.pad(activation_7, (1, 1, 1, 1))\n",
    "        audio_embedding_layer = self.audio_embedding_layer(audio_embedding_layer_pad)\n",
    "        max_pooling2d_4 = F.max_pool2d(audio_embedding_layer, kernel_size=(4, 8), stride=(4, 8), padding=0, ceil_mode=False)\n",
    "        return max_pooling2d_4\n",
    "\n",
    "\n",
    "    def __batch_normalization(self, dim, name, **kwargs):\n",
    "        if   dim == 0 or dim == 1:  layer = nn.BatchNorm1d(**kwargs)\n",
    "        elif dim == 2:  layer = nn.BatchNorm2d(**kwargs)\n",
    "        elif dim == 3:  layer = nn.BatchNorm3d(**kwargs)\n",
    "        else:           raise NotImplementedError()\n",
    "\n",
    "        if 'scale' in self.__weights_dict[name]:\n",
    "            layer.state_dict()['weight'].copy_(torch.from_numpy(self.__weights_dict[name]['scale']))\n",
    "        else:\n",
    "            layer.weight.data.fill_(1)\n",
    "\n",
    "        if 'bias' in self.__weights_dict[name]:\n",
    "            layer.state_dict()['bias'].copy_(torch.from_numpy(self.__weights_dict[name]['bias']))\n",
    "        else:\n",
    "            layer.bias.data.fill_(0)\n",
    "\n",
    "        layer.state_dict()['running_mean'].copy_(torch.from_numpy(self.__weights_dict[name]['mean']))\n",
    "        layer.state_dict()['running_var'].copy_(torch.from_numpy(self.__weights_dict[name]['var']))\n",
    "        return layer\n",
    "\n",
    "    def __conv(self, dim, name, **kwargs):\n",
    "        if   dim == 1:  layer = nn.Conv1d(**kwargs)\n",
    "        elif dim == 2:  layer = nn.Conv2d(**kwargs)\n",
    "        elif dim == 3:  layer = nn.Conv3d(**kwargs)\n",
    "        else:           raise NotImplementedError()\n",
    "\n",
    "        layer.state_dict()['weight'].copy_(torch.from_numpy(self.__weights_dict[name]['weights']))\n",
    "        if 'bias' in self.__weights_dict[name]:\n",
    "            layer.state_dict()['bias'].copy_(torch.from_numpy(self.__weights_dict[name]['bias']))\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task5Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(1)\n",
    "\n",
    "        self.openl3 = KitModel('./openl3_no_mel_layer_pytorch_weights')\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, 256), nn.ReLU(), nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 1, 3, 2) # keras model had (92, 128, 199, 1) like shape\n",
    "        x = self.bn(x)\n",
    "        x = self.openl3(x)\n",
    "        x = x.max(dim=-1)[0].max(dim=-1)[0]\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_files = glob('../../data/audio-eval/*.wav')\n",
    "eval_files = [os.path.basename(x) for x in eval_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([\n",
    "        np.expand_dims(np.load('../../data/logmelspec-eval/{}.npy'.format(x)).T[:635, :], axis=0)\n",
    "        for x in eval_files])\n",
    "X = X[:, None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_means = np.load('../../data/channel_means.npy')\n",
    "channel_stds = np.load('../../data/channel_stds.npy')\n",
    "X = (X - channel_means) / channel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx, ...]\n",
    "        i = np.random.randint(sample.shape[1])\n",
    "        sample = torch.cat([\n",
    "                sample[:, i:, :],\n",
    "                sample[:, :i, :]],\n",
    "                dim=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(torch.Tensor(X))\n",
    "loader = DataLoader(dataset, 16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = True\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Task5Model(31).to(device)\n",
    "model.load_state_dict(torch.load('./model_system1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "for _ in range(10):\n",
    "    preds = []\n",
    "    for inputs in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                model = model.eval()\n",
    "                outputs = model(inputs)\n",
    "                preds.append(outputs.detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    preds = (1 / (1 + np.exp(-preds)))\n",
    "    all_preds.append(preds)\n",
    "tmp = all_preds[0]\n",
    "for x in all_preds[1:]:\n",
    "    tmp += x\n",
    "tmp = tmp / 10\n",
    "preds = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(\n",
    "    preds, columns=[\n",
    "        '1_engine', '2_machinery-impact', '3_non-machinery-impact',\n",
    "        '4_powered-saw', '5_alert-signal', '6_music', '7_human-voice', '8_dog',\n",
    "        '1-1_small-sounding-engine', '1-2_medium-sounding-engine',\n",
    "        '1-3_large-sounding-engine', '2-1_rock-drill', '2-2_jackhammer',\n",
    "        '2-3_hoe-ram', '2-4_pile-driver', '3-1_non-machinery-impact',\n",
    "        '4-1_chainsaw', '4-2_small-medium-rotating-saw',\n",
    "        '4-3_large-rotating-saw', '5-1_car-horn', '5-2_car-alarm', '5-3_siren',\n",
    "        '5-4_reverse-beeper', '6-1_stationary-music', '6-2_mobile-music',\n",
    "        '6-3_ice-cream-truck', '7-1_person-or-small-group-talking',\n",
    "        '7-2_person-or-small-group-shouting', '7-3_large-crowd',\n",
    "        '7-4_amplified-speech', '8-1_dog-barking-whining'])\n",
    "output_df['audio_filename'] = pd.Series(eval_files, index=output_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_in_order = [\n",
    "    \"audio_filename\", \"1-1_small-sounding-engine\",\n",
    "    \"1-2_medium-sounding-engine\", \"1-3_large-sounding-engine\",\n",
    "    \"2-1_rock-drill\",\n",
    "    \"2-2_jackhammer\", \"2-3_hoe-ram\", \"2-4_pile-driver\",\n",
    "    \"3-1_non-machinery-impact\",\n",
    "    \"4-1_chainsaw\", \"4-2_small-medium-rotating-saw\",\n",
    "    \"4-3_large-rotating-saw\",\n",
    "    \"5-1_car-horn\", \"5-2_car-alarm\", \"5-3_siren\", \"5-4_reverse-beeper\",\n",
    "    \"6-1_stationary-music\",\n",
    "    \"6-2_mobile-music\", \"6-3_ice-cream-truck\",\n",
    "    \"7-1_person-or-small-group-talking\",\n",
    "    \"7-2_person-or-small-group-shouting\", \"7-3_large-crowd\",\n",
    "    \"7-4_amplified-speech\",\n",
    "    \"8-1_dog-barking-whining\", \"1_engine\", \"2_machinery-impact\",\n",
    "    \"3_non-machinery-impact\", \"4_powered-saw\", \"5_alert-signal\",\n",
    "    \"6_music\", \"7_human-voice\", \"8_dog\"]\n",
    "output_df = output_df.loc[:, cols_in_order]\n",
    "\n",
    "output_df.to_csv('submission-system-1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
