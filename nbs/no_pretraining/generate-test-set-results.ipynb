{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/io/_video_opt.py:17: UserWarning: video reader based on ffmpeg c++ ops not available\n",
      "  warnings.warn(\"video reader based on ffmpeg c++ ops not available\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task5Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.bw2col = nn.Sequential(\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Conv2d(1, 10, 1, padding=0), nn.ReLU(),\n",
    "            nn.Conv2d(10, 3, 1, padding=0), nn.ReLU())\n",
    "\n",
    "        self.mv2 = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(1280, 512), nn.ReLU(), nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bw2col(x)\n",
    "        x = self.mv2.features(x)\n",
    "        x = x.max(dim=-1)[0].max(dim=-1)[0]\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_files = glob('../../data/audio-eval/*.wav')\n",
    "eval_files = [os.path.basename(x) for x in eval_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([\n",
    "        np.expand_dims(np.load('../../data/logmelspec-eval/{}.npy'.format(x)).T[:635, :], axis=0)\n",
    "        for x in eval_files])\n",
    "X = X[:, None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_means = np.load('../../data/channel_means.npy')\n",
    "channel_stds = np.load('../../data/channel_stds.npy')\n",
    "X = (X - channel_means) / channel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx, ...]\n",
    "        i = np.random.randint(sample.shape[1])\n",
    "        sample = torch.cat([\n",
    "                sample[:, i:, :],\n",
    "                sample[:, :i, :]],\n",
    "                dim=1)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(torch.Tensor(X))\n",
    "loader = DataLoader(dataset, 64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = True\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Task5Model(31).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols = [\n",
    "        '1_engine', '2_machinery-impact', '3_non-machinery-impact',\n",
    "        '4_powered-saw', '5_alert-signal', '6_music', '7_human-voice', '8_dog',\n",
    "        '1-1_small-sounding-engine', '1-2_medium-sounding-engine',\n",
    "        '1-3_large-sounding-engine', '2-1_rock-drill', '2-2_jackhammer',\n",
    "        '2-3_hoe-ram', '2-4_pile-driver', '3-1_non-machinery-impact',\n",
    "        '4-1_chainsaw', '4-2_small-medium-rotating-saw',\n",
    "        '4-3_large-rotating-saw', '5-1_car-horn', '5-2_car-alarm', '5-3_siren',\n",
    "        '5-4_reverse-beeper', '6-1_stationary-music', '6-2_mobile-music',\n",
    "        '6-3_ice-cream-truck', '7-1_person-or-small-group-talking',\n",
    "        '7-2_person-or-small-group-shouting', '7-3_large-crowd',\n",
    "        '7-4_amplified-speech', '8-1_dog-barking-whining']\n",
    "cols_in_order = [\n",
    "    \"audio_filename\", \"1-1_small-sounding-engine\",\n",
    "    \"1-2_medium-sounding-engine\", \"1-3_large-sounding-engine\",\n",
    "    \"2-1_rock-drill\",\n",
    "    \"2-2_jackhammer\", \"2-3_hoe-ram\", \"2-4_pile-driver\",\n",
    "    \"3-1_non-machinery-impact\",\n",
    "    \"4-1_chainsaw\", \"4-2_small-medium-rotating-saw\",\n",
    "    \"4-3_large-rotating-saw\",\n",
    "    \"5-1_car-horn\", \"5-2_car-alarm\", \"5-3_siren\", \"5-4_reverse-beeper\",\n",
    "    \"6-1_stationary-music\",\n",
    "    \"6-2_mobile-music\", \"6-3_ice-cream-truck\",\n",
    "    \"7-1_person-or-small-group-talking\",\n",
    "    \"7-2_person-or-small-group-shouting\", \"7-3_large-crowd\",\n",
    "    \"7-4_amplified-speech\",\n",
    "    \"8-1_dog-barking-whining\", \"1_engine\", \"2_machinery-impact\",\n",
    "    \"3_non-machinery-impact\", \"4_powered-saw\", \"5_alert-signal\",\n",
    "    \"6_music\", \"7_human-voice\", \"8_dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_pretrained_run1_model\n",
      "no_pretrained_run2_model\n",
      "no_pretrained_run3_model\n",
      "with_pretrained_run1_model\n",
      "with_pretrained_run2_model\n",
      "with_pretrained_run3_model\n"
     ]
    }
   ],
   "source": [
    "for a_model_file in sorted(glob('*_model')):\n",
    "    print(a_model_file)\n",
    "    \n",
    "    model.load_state_dict(torch.load(a_model_file))\n",
    "    \n",
    "    all_preds = []\n",
    "    for _ in range(10):\n",
    "        preds = []\n",
    "        for inputs in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    model = model.eval()\n",
    "                    outputs = model(inputs)\n",
    "                    preds.append(outputs.detach().cpu().numpy())\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        preds = (1 / (1 + np.exp(-preds)))\n",
    "        all_preds.append(preds)\n",
    "    tmp = all_preds[0]\n",
    "    for x in all_preds[1:]:\n",
    "        tmp += x\n",
    "    tmp = tmp / 10\n",
    "    preds = tmp\n",
    "\n",
    "    output_df = pd.DataFrame(preds, columns=output_cols)\n",
    "    output_df['audio_filename'] = pd.Series(eval_files, index=output_df.index)\n",
    "    output_df = output_df.loc[:, cols_in_order]\n",
    "    \n",
    "    a_file = os.path.basename(a_model_file)\n",
    "    output_df.to_csv('{}.csv'.format(a_file), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
