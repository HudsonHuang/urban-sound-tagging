{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sai/anaconda3/envs/py37/lib/python3.7/site-packages/torchvision/io/_video_opt.py:17: UserWarning: video reader based on ffmpeg c++ ops not available\n",
      "  warnings.warn(\"video reader based on ffmpeg c++ ops not available\")\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from RandomErasing import RandomErasing\n",
    "\n",
    "import torchvision.models\n",
    "from torchvision import transforms\n",
    "\n",
    "from albumentations import Compose, ShiftScaleRotate, GridDistortion\n",
    "from albumentations.pytorch import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, unknown_to_known):\n",
    "    df = df.reset_index()\n",
    "    df['slno'] = df.assign(slno=1).groupby('audio_filename')['slno'].cumsum()\n",
    "    df.set_index(['audio_filename', 'slno'], inplace=True)\n",
    "\n",
    "    df_unknown = df.copy().loc[:, list(unknown_to_known.keys())]\n",
    "    df.drop(columns=list(unknown_to_known.keys()), inplace=True)\n",
    "\n",
    "    y_mask = df.copy()\n",
    "    y_mask.loc[:, :] = 1\n",
    "    for unknown, known in unknown_to_known.items():\n",
    "        y_mask.loc[\n",
    "            df_unknown[unknown] > 0.5,\n",
    "            known\n",
    "        ] = 0\n",
    "\n",
    "    df = df.swaplevel(i=1, j=0, axis=0).sort_index()\n",
    "\n",
    "    y_mask = y_mask.swaplevel(i=1, j=0, axis=0).sort_index()\n",
    "\n",
    "    y = np.concatenate([\n",
    "        df.loc[[1], :].values[..., np.newaxis],\n",
    "        df.loc[[2], :].values[..., np.newaxis],\n",
    "        df.loc[[3], :].values[..., np.newaxis]\n",
    "    ], axis=2)\n",
    "\n",
    "    y_mask = np.concatenate([\n",
    "        y_mask.loc[[1], :].values[..., np.newaxis],\n",
    "        y_mask.loc[[2], :].values[..., np.newaxis],\n",
    "        y_mask.loc[[3], :].values[..., np.newaxis]\n",
    "    ], axis=2)\n",
    "\n",
    "    X = np.concatenate([\n",
    "        np.expand_dims(np.load('../../data/logmelspec/{}.npy'.format(x)).T[:635, :], axis=0)\n",
    "        for x in df.loc[[1], :].reset_index(1).audio_filename.tolist()])\n",
    "    X = np.expand_dims(X, axis=1)\n",
    "\n",
    "    return X, y, y_mask\n",
    "\n",
    "\n",
    "random_erasing = RandomErasing()\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X, y, weights, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights\n",
    "        self.transform = transform\n",
    "        self.pil = transforms.ToPILImage()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx, ...]\n",
    "\n",
    "        if self.transform:\n",
    "            # min-max transformation\n",
    "            this_min = sample.min()\n",
    "            this_max = sample.max()\n",
    "            sample = (sample - this_min) / (this_max - this_min)\n",
    "\n",
    "            # randomly cycle the file\n",
    "            i = np.random.randint(sample.shape[1])\n",
    "            sample = torch.cat([\n",
    "                sample[:, i:, :],\n",
    "                sample[:, :i, :]],\n",
    "                dim=1)\n",
    "\n",
    "            # apply albumentations transforms\n",
    "            sample = np.array(self.pil(sample))\n",
    "            sample = self.transform(image=sample)\n",
    "            sample = sample['image']\n",
    "            sample = sample[None, :, :].permute(0, 2, 1)\n",
    "\n",
    "            # apply random erasing\n",
    "            sample = random_erasing(sample.clone().detach())\n",
    "\n",
    "            # revert min-max transformation\n",
    "            sample = (sample * (this_max - this_min)) + this_min\n",
    "\n",
    "        return sample, self.y[idx, ...], self.weights[idx, ...]\n",
    "\n",
    "\n",
    "class Task5Model(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.bw2col = nn.Sequential(\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Conv2d(1, 10, 1, padding=0), nn.ReLU(),\n",
    "            nn.Conv2d(10, 3, 1, padding=0), nn.ReLU())\n",
    "\n",
    "        self.mv2 = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(1280, 512), nn.ReLU(), nn.BatchNorm1d(512),\n",
    "            nn.Linear(512, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bw2col(x)\n",
    "        x = self.mv2.features(x)\n",
    "        x = x.max(dim=-1)[0].max(dim=-1)[0]\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "with open('../../data/metadata.pkl', 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "unknown_to_known = (\n",
    "    pd.merge(metadata['taxonomy_df'].loc[lambda x: x.fine_id == 'X', ['fine', 'coarse']],\n",
    "             metadata['taxonomy_df'].loc[lambda x: x.fine_id != 'X', ['fine', 'coarse']],\n",
    "             on='coarse', how='inner')\n",
    "    .drop(columns='coarse')\n",
    "    .groupby('fine_x')['fine_y']\n",
    "    .apply(lambda x: list(x)).to_dict())\n",
    "known_labels = metadata['taxonomy_df'].loc[lambda x: x.fine_id != 'X'].fine.tolist()\n",
    "\n",
    "train_df = pd.concat([metadata['coarse_train'], metadata['fine_train']], axis=1, sort=True)\n",
    "valid_df = pd.concat([metadata['coarse_test'], metadata['fine_test']], axis=1, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual correction for one data point\n",
    "train_df.loc[(train_df.sum(axis=1) == 37).copy(), :] = 0\n",
    "valid_df.loc[(valid_df.sum(axis=1) == 37).copy(), :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, train_y_mask = prepare_data(train_df, unknown_to_known)\n",
    "valid_X, valid_y, valid_y_mask = prepare_data(valid_df, unknown_to_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channel wise normalization\n",
    "channel_means = train_X.reshape(-1, 128).mean(axis=0).reshape(1, 1, 1, -1)\n",
    "channel_stds = train_X.reshape(-1, 128).std(axis=0).reshape(1, 1, 1, -1)\n",
    "train_X = (train_X - channel_means) / channel_stds\n",
    "valid_X = (valid_X - channel_means) / channel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data augmentation transformations\n",
    "albumentations_transform = Compose([\n",
    "    ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0.5),\n",
    "    GridDistortion(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the datasets and the dataloaders\n",
    "train_dataset = AudioDataset(torch.Tensor(train_X),\n",
    "                             torch.Tensor(train_y),\n",
    "                             torch.Tensor(train_y_mask),\n",
    "                             albumentations_transform)\n",
    "valid_dataset = AudioDataset(torch.Tensor(valid_X),\n",
    "                             torch.Tensor(valid_y),\n",
    "                             torch.Tensor(valid_y_mask),\n",
    "                             None)\n",
    "\n",
    "val_loader = DataLoader(valid_dataset, 64, shuffle=False)\n",
    "train_loader_1 = DataLoader(train_dataset, 64, shuffle=True)\n",
    "train_loader_2 = DataLoader(train_dataset, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Define the device to be used\n",
    "cuda = True\n",
    "device = torch.device('cuda:0' if cuda else 'cpu')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = Task5Model(31).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer, scheduler and loss criteria\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, amsgrad=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "0.6332479634800473 0.47939824206488474\n",
      "Epoch:  1\n",
      "0.32351818600216425 0.17857412355286734\n",
      "Epoch:  2\n",
      "0.17805396221779488 0.14592521211930684\n",
      "Epoch:  3\n",
      "0.16274138479619413 0.16960285604000092\n",
      "Epoch:  4\n",
      "0.1590438388489388 0.13001582665102823\n",
      "Epoch:  5\n",
      "0.15638854898310997 0.14092459636075155\n",
      "Epoch:  6\n",
      "0.1558752164647386 0.13305165831531798\n",
      "Epoch:  7\n",
      "0.15303590249370885 0.1284072037254061\n",
      "Epoch:  8\n",
      "0.15405449835029808 0.1299319490790367\n",
      "Epoch:  9\n",
      "0.15295552844936783 0.13481496274471283\n",
      "Epoch:  10\n",
      "0.1533412502424137 0.12972551158496312\n",
      "Epoch:  11\n",
      "0.15362192448732015 0.15571873847927367\n",
      "Epoch:  12\n",
      "0.15083124750369303 0.1284335862312998\n",
      "Epoch:  13\n",
      "0.1502585072775145 0.13159883660929544\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch:  14\n",
      "0.14855290909071225 0.12304842791387013\n",
      "Epoch:  15\n",
      "0.1478096874984535 0.12170381098985672\n",
      "Epoch:  16\n",
      "0.1469171470081484 0.12157972476312093\n",
      "Epoch:  17\n",
      "0.1455977756429363 0.1210864367229598\n",
      "Epoch:  18\n",
      "0.14520413408408295 0.12087375457797732\n",
      "Epoch:  19\n",
      "0.1463287992251886 0.12115283523287092\n",
      "Epoch:  20\n",
      "0.14613631810690905 0.12078140143837247\n",
      "Epoch:  21\n",
      "0.1448694459489874 0.1205425847853933\n",
      "Epoch:  22\n",
      "0.14448375315279574 0.12067848443984985\n",
      "Epoch:  23\n",
      "0.1449465203929592 0.12074542897088188\n",
      "Epoch:  24\n",
      "0.14506403739387924 0.1201176153762\n",
      "Epoch:  25\n",
      "0.14398880383452853 0.11974133657557624\n",
      "Epoch:  26\n",
      "0.14339814395517916 0.12076693028211594\n",
      "Epoch:  27\n",
      "0.14523745106684194 0.11964190006256104\n",
      "Epoch:  28\n",
      "0.14575115450330683 0.12012100645474025\n",
      "Epoch:  29\n",
      "0.1433806761696532 0.11971405467816762\n",
      "Epoch:  30\n",
      "0.14402445264764735 0.12009876221418381\n",
      "Epoch:  31\n",
      "0.14431386863863147 0.1199342810681888\n",
      "Epoch:  32\n",
      "0.1437341220475532 0.11953749294791903\n",
      "Epoch:  33\n",
      "0.14438592984869675 0.12049810801233564\n",
      "Epoch:  34\n",
      "0.14283381966320244 0.11967359057494573\n",
      "Epoch:  35\n",
      "0.1435130596966357 0.11976411512919835\n",
      "Epoch:  36\n",
      "0.1429838143490456 0.11929973001991\n",
      "Epoch:  37\n",
      "0.14328824426676776 0.1191604073558535\n",
      "Epoch:  38\n",
      "0.14401589897838799 0.12009517529181071\n",
      "Epoch:  39\n",
      "0.14378285206652977 0.11951530511890139\n",
      "Epoch:  40\n",
      "0.14250318585215388 0.11931223315852028\n",
      "Epoch:  41\n",
      "0.1414238121863958 0.11963310518435069\n",
      "Epoch:  42\n",
      "0.14294041532116966 0.12012095217193876\n",
      "Epoch:  43\n",
      "0.14068852123376485 0.12002912483045033\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch:  44\n",
      "0.14273255137172905 0.11957359313964844\n",
      "Epoch:  45\n",
      "0.14181961340678706 0.11955961797918592\n",
      "Epoch:  46\n",
      "0.1428267464444444 0.11951304759298052\n",
      "Epoch:  47\n",
      "0.14292738204066818 0.11947536255632128\n",
      "Epoch:  48\n",
      "0.14212419535662676 0.1195983961224556\n",
      "Epoch:  49\n",
      "0.14181617264812058 0.11954322350876671\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch:  50\n",
      "0.14149804373045224 0.11975925415754318\n",
      "Epoch:  51\n",
      "0.1417821126209723 0.1193231897694724\n",
      "Epoch:  52\n",
      "0.14132721802672824 0.1197093693273408\n",
      "Epoch:  53\n",
      "0.14079161149424477 0.11950293928384781\n",
      "Epoch:  54\n",
      "0.1406244051617545 0.11939686217478343\n",
      "Epoch:  55\n",
      "0.14051409327500575 0.11963909864425659\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch:  56\n",
      "0.14070545499389236 0.11943378299474716\n",
      "Epoch:  57\n",
      "0.1421497959542919 0.11933978008372444\n",
      "Epoch:  58\n",
      "0.1415242262788721 0.11956335604190826\n",
      "Epoch:  59\n",
      "0.14071698768718824 0.11941635715109962\n",
      "Epoch:  60\n",
      "0.14152871676393458 0.11921065832887377\n",
      "Epoch:  61\n",
      "0.14211412458806424 0.11943210874285017\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch:  62\n",
      "0.14147161269510114 0.11929138749837875\n",
      "Epoch:  63\n",
      "0.14165674673544393 0.11937760774578367\n",
      "Epoch:  64\n",
      "0.14119982719421387 0.11920657860381263\n",
      "Epoch:  65\n",
      "0.1422930621617549 0.11932649782725743\n",
      "Epoch:  66\n",
      "0.14180175074048945 0.11952389883143562\n",
      "Epoch:  67\n",
      "0.14172617487005285 0.11944217447723661\n",
      "Epoch:  68\n",
      "0.14039576939634374 0.11926190448658806\n",
      "Epoch:  69\n",
      "0.14088201885287827 0.11946162049259458\n",
      "Epoch:  70\n",
      "0.1411494427436107 0.11962324380874634\n",
      "Epoch:  71\n",
      "0.1417370478849153 0.1195976585149765\n",
      "Epoch:  72\n",
      "0.14118753373622894 0.11930152241672788\n",
      "Epoch:  73\n",
      "0.14111192886893814 0.11935411074331828\n",
      "Epoch:  74\n",
      "0.14085605418359912 0.1194142826965877\n",
      "Epoch:  75\n",
      "0.140631862991565 0.11940358792032514\n",
      "Epoch:  76\n",
      "0.1425317892351666 0.1195127431835447\n",
      "Epoch:  77\n",
      "0.14096658495632378 0.11947139352560043\n",
      "Epoch:  78\n",
      "0.14132146899764603 0.11942957873855319\n",
      "Epoch:  79\n",
      "0.14122981799615397 0.11945259890386037\n",
      "Epoch:  80\n",
      "0.14275001955998912 0.11958404843296323\n",
      "Epoch:  81\n",
      "0.1418883031284487 0.11932632220642907\n",
      "Epoch:  82\n",
      "0.14171586689111348 0.11939081975391932\n",
      "Epoch:  83\n",
      "0.14029901978131887 0.11944537609815598\n",
      "Epoch:  84\n",
      "0.1409882991700559 0.11979597487619945\n",
      "Epoch:  85\n",
      "0.1422286033630371 0.11915912479162216\n",
      "Epoch:  86\n",
      "0.1403165205910399 0.11955126162086215\n",
      "Epoch:  87\n",
      "0.1417052798174523 0.11950112879276276\n",
      "Epoch:  88\n",
      "0.14156252427681074 0.11942778953484126\n",
      "Epoch:  89\n",
      "0.14112507692865423 0.11962518521717616\n",
      "Epoch:  90\n",
      "0.14146799412933556 0.11974457651376724\n",
      "Epoch:  91\n",
      "0.14166275270887324 0.11940982724939074\n",
      "Epoch:  92\n",
      "0.1417176300609434 0.1194383714880262\n",
      "Epoch:  93\n",
      "0.14253138730654846 0.11935859812157494\n",
      "Epoch:  94\n",
      "0.14107305858586286 0.11941503307649068\n",
      "Epoch:  95\n",
      "0.14126711882449486 0.11952992847987584\n",
      "Epoch:  96\n",
      "0.14065760655983076 0.1197558024099895\n",
      "Epoch:  97\n",
      "0.14168826752417796 0.11950052848884038\n",
      "Epoch:  98\n",
      "0.14095062379901474 0.11928970153842654\n",
      "Epoch:  99\n",
      "0.14153627165265986 0.1195683479309082\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "train_loss_hist = []\n",
    "valid_loss_hist = []\n",
    "lowest_val_loss = np.inf\n",
    "epochs_without_new_lowest = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('Epoch: ', i)\n",
    "\n",
    "    this_epoch_train_loss = 0\n",
    "    for i1, i2 in zip(train_loader_1, train_loader_2):\n",
    "\n",
    "        # mixup the inputs ---------\n",
    "        alpha = 1\n",
    "        mixup_vals = np.random.beta(alpha, alpha, i1[0].shape[0])\n",
    "\n",
    "        lam = torch.Tensor(mixup_vals.reshape(mixup_vals.shape[0], 1, 1, 1))\n",
    "        inputs = (lam * i1[0]) + ((1 - lam) * i2[0])\n",
    "\n",
    "        lam = torch.Tensor(mixup_vals.reshape(mixup_vals.shape[0], 1, 1))\n",
    "        labels = (lam * i1[1]) + ((1 - lam) * i2[1])\n",
    "        masks = (lam * i1[2]) + ((1 - lam) * i2[2])\n",
    "        # mixup ends ----------\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            model = model.train()\n",
    "            outputs = model(inputs)\n",
    "            # calculate loss for each set of annotations\n",
    "            loss_0 = criterion(outputs, labels[:, :, 0]) * masks[:, :, 0]\n",
    "            loss_1 = criterion(outputs, labels[:, :, 1]) * masks[:, :, 1]\n",
    "            loss_2 = criterion(outputs, labels[:, :, 2]) * masks[:, :, 2]\n",
    "            loss = (loss_0.sum() + loss_1.sum() + loss_2.sum()) / masks.sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            this_epoch_train_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "    this_epoch_valid_loss = 0\n",
    "    for inputs, labels, masks in val_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(False):\n",
    "            model = model.eval()\n",
    "            outputs = model(inputs)\n",
    "            loss_0 = criterion(outputs, labels[:, :, 0]) * masks[:, :, 0]\n",
    "            loss_1 = criterion(outputs, labels[:, :, 1]) * masks[:, :, 1]\n",
    "            loss_2 = criterion(outputs, labels[:, :, 2]) * masks[:, :, 2]\n",
    "            loss = (loss_0.sum() + loss_1.sum() + loss_2.sum()) / masks.sum()\n",
    "            this_epoch_valid_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "    this_epoch_train_loss /= len(train_loader_1)\n",
    "    this_epoch_valid_loss /= len(val_loader)\n",
    "\n",
    "    train_loss_hist.append(this_epoch_train_loss)\n",
    "    valid_loss_hist.append(this_epoch_valid_loss)\n",
    "\n",
    "    if this_epoch_valid_loss < lowest_val_loss:\n",
    "        lowest_val_loss = this_epoch_valid_loss\n",
    "        torch.save(model.state_dict(), './with_pretrained_run3_model')\n",
    "        epochs_without_new_lowest = 0\n",
    "    else:\n",
    "        epochs_without_new_lowest += 1\n",
    "\n",
    "    print(this_epoch_train_loss, this_epoch_valid_loss)\n",
    "\n",
    "    scheduler.step(this_epoch_valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('with_pretrained_run3_hist.pkl', 'wb') as f:\n",
    "    pickle.dump((train_loss_hist, valid_loss_hist), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
