@InProceedings{cJones2003,
  author = 	 "Jones, C.D. and Smith, A.B. and Roberts, E.F.",
  TITLE   =      "A sample paper in conference proceedings",
  booktitle =    "Proc. IEEE ICASSP",
  year = 	 "2003",
  volume = 	 "II",
  pages = 	 "803-806"
}

@Book{eWilliams1999,
  author =       {E.G. Williams},
  title =        {Fourier Acoustics: Sound Radiation and Nearfield Acoustic Holography},
  publisher =    {Academic Press},
  year =         {1999},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  address =      {London, UK},
  OPTedition =   {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@Article{aSmith2000,
  author =       "A.B. Smith and C.D. Jones and E.F. Roberts",
  title =        "A sample paper in journals",
  journal = 	 "IEEE Trans. Signal Process.",
  year = 	 "2000",
  volume = 	 "62",
  pages = 	 "291-294",
  month = 	 "Jan."
}

@Misc{dcase2019web,
  howpublished = {\url{http://dcase.community/workshop2019/}}
}

@Misc{ieeedjo,
  howpublished = {\url{http://www.ieee.org/portal/pages/pubs/confpubcenter/register.html}}
}


@Misc{IEEEPDFSpec,
  howpublished = {{PDF} specification for {IEEE} {X}plore,
    \url{https://www2.securecms.com/ICASSP2015/papers/PaperFormat/Author-PDF-Guide-V32.pdf}}
}


@Misc{dcase2019task5,
  howpublished = {\url{http://dcase.community/challenge2019/task-urban-sound-tagging}}
}

@Misc{mv2weights,
  howpublished = {\url{https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md}}
}

@inproceedings{sandler2018mobilenetv2,
  title={Mobilenetv2: Inverted residuals and linear bottlenecks},
  author={Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4510--4520},
  year={2018}
}

@article{bello2018sonyc,
  title={SONYC: A system for the monitoring, analysis and mitigation of urban noise pollution},
  author={Bello, Juan Pablo and Silva, Claudio and Nov, Oded and DuBois, R Luke and Arora, Anish and Salamon, Justin and Mydlarz, Charles and Doraiswamy, Harish},
  journal={arXiv preprint arXiv:1805.00889},
  year={2018}
}

@article{Bello2019sonyc,
    author = "Bello, Juan P. and Silva, Claudio and Nov, Oded and Dubois, R. Luke and Arora, Anish and Salamon, Justin and Mydlarz, Charles and Doraiswamy, Harish",
    doi = "10.1145/3224204",
    title = "SONYC: A System for Monitoring, Analyzing, and Mitigating Urban Noise Pollution",
    abstract = "Noise is unwanted or harmful sound from environmental sources, including traffic, construction, industrial, and social activity. Noise pollution is one of the topmost quality-of-life concerns for urban residents in the U.S., with more than 70 million people nationwide exposed to noise levels beyond the limit the U.S. Environmental Protection Agency (EPA) considers harmful.12 Such levels have proven effects on health, including sleep disruption, hypertension, heart disease, and hearing loss.5,11,12 In addition, there is evidence of harmful effects on educational performance, with studies showing noise pollution causing learning and cognitive impairment in children, resulting in decreased memory capacity, reading skills, and test scores.",
    number = "2",
    month = "Feb",
    volume = "62",
    pages = "68-77",
    year = "2019",
    journal = "Communications of the ACM"
}

@techreport{Gousseau2019,
    Author = "Gousseau, Clément",
    abstract = "A model of urban sound tagging is presented (Task 5 of DCASE 2019 [1][2]). The task is to detect activities from 10-seconds audio segments recorded in the streets of New York City (SONYC dataset). The model is based on the model presented in the book Hands-On Transfer Learning with Python [3] which does urban sound classification for the UrbanSound dataset. This model has been adapted and optimized to address the task 5 of DCASE2019. It achieved a AUPRC of 82.6 for the coarse-grained model where the baseline achieves an AUPRC of 76.2.",
    month = "September",
    year = "2019",
    title = "{VGG} {CNN} for Urban Sound Tagging",
    institution = "DCASE2019 Challenge"
}

@inproceedings{gemmeke2017audio,
  title={Audio set: An ontology and human-labeled dataset for audio events},
  author={Gemmeke, Jort F and Ellis, Daniel PW and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R Channing and Plakal, Manoj and Ritter, Marvin},
  booktitle={2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={776--780},
  year={2017},
  organization={IEEE}
}

@inproceedings{hershey2017cnn,
  title={CNN architectures for large-scale audio classification},
  author={Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel PW and Gemmeke, Jort F and Jansen, Aren and Moore, R Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A and Seybold, Bryan and others},
  booktitle={2017 ieee international conference on acoustics, speech and signal processing (icassp)},
  pages={131--135},
  year={2017},
  organization={IEEE}
}

@article{kong2019cross,
  title={Cross-task learning for audio tagging, sound event detection and spatial localization: DCASE 2019 baseline systems},
  author={Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Xu, Yong and Wang, Wenwu and Plumbley, Mark D},
  journal={arXiv preprint arXiv:1904.03476},
  year={2019}
}

@article{salamon2017deep,
  title={Deep convolutional neural networks and data augmentation for environmental sound classification},
  author={Salamon, Justin and Bello, Juan Pablo},
  journal={IEEE Signal Processing Letters},
  volume={24},
  number={3},
  pages={279--283},
  year={2017},
  publisher={IEEE}
}

@misc{brian_mcfee_2019_2564164,
  author       = {Brian McFee and
                  Matt McVicar and
                  Stefan Balke and
                  Vincent Lostanlen and
                  Carl Thomé and
                  Colin Raffel and
                  Dana Lee and
                  Kyungyun Lee and
                  Oriol Nieto and
                  Frank Zalkow and
                  Dan Ellis and
                  Eric Battenberg and
                  Ryuichi Yamamoto and
                  Josh Moore and
                  Ziyao Wei and
                  Rachel Bittner and
                  Keunwoo Choi and
                  nullmightybofo and
                  Pius Friesch and
                  Fabian-Robert Stöter and
                  Thassilo and
                  Matt Vollrath and
                  Siddhartha Kumar Golu and
                  nehz and
                  Simon Waloschek and
                  Seth and
                  Rimvydas Naktinis and
                  Douglas Repetto and
                  Curtis "Fjord" Hawthorne and
                  CJ Carr},
  title        = {librosa/librosa: 0.6.3},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2564164},
  url          = {https://doi.org/10.5281/zenodo.2564164}
}

@article{howard2019searching,
  title={Searching for MobileNetV3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  journal={arXiv preprint arXiv:1905.02244},
  year={2019}
}

@article{2018arXiv180906839B,
    author = {A. Buslaev, A. Parinov, E. Khvedchenya, V.~I. Iglovikov and A.~A. Kalinin},
     title = "{Albumentations: fast and flexible image augmentations}",
   journal = {ArXiv e-prints},
    eprint = {1809.06839},
      year = 2018      
}

@article{zhong2017random,
  title={Random erasing data augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  journal={arXiv preprint arXiv:1708.04896},
  year={2017}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{paszke2017automatic,
  title={Automatic Differentiation in {PyTorch}},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle={NIPS Autodiff Workshop},
  year={2017}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@techreport{Cartwright2019,
    Author = "Cartwright, Mark and Cramer, Jason and Mendez Mendez, Ana Elisa and Wu, Ho-Hsiang and Lostanlen, Vincent and Bello, Juan P. and Salamon, Justin",
    abstract = "SONYC Urban Sound Tagging (SONYC-UST) is a dataset for the development and evaluation of machine listening systems for realistic urban noise monitoring. The audio was recorded from an acoustic sensor network named ``Sounds of New York City'' (SONYC). Via the Zooniverse citizen science platform, volunteers tagged the presence of 23 classes that were priorly chosen in consultation with the New York City Department of Environmental Protection. These 23 fine-grained classes can be grouped into eight coarse-grained classes.",
    month = "September",
    year = "2019",
    title = "Sonyc Urban Sound Tagging: A Multilabel Dataset From an Urban Acoustic Sensor Network",
    institution = "DCASE2019 Challenge"
}

@article{eigen2013understanding,
  title={Understanding deep architectures using a recursive convolutional network},
  author={Eigen, David and Rolfe, Jason and Fergus, Rob and LeCun, Yann},
  journal={arXiv preprint arXiv:1312.1847},
  year={2013}
}

@inproceedings{seide2011conversational,
  title={Conversational speech transcription using context-dependent deep neural networks},
  author={Seide, Frank and Li, Gang and Yu, Dong},
  booktitle={Twelfth annual conference of the international speech communication association},
  year={2011}
}

@article{houix2012lexical,
  title={A lexical analysis of environmental sound categories.},
  author={Houix, Olivier and Lemaitre, Guillaume and Misdariis, Nicolas and Susini, Patrick and Urdapilleta, Isabel},
  journal={Journal of Experimental Psychology: Applied},
  volume={18},
  number={1},
  pages={52},
  year={2012},
  publisher={American Psychological Association}
}

@inproceedings{Jeong2018,
    author = "Jeong, Il-Young and Lim, Hyungui",
    title = "Audio tagging system using densely connected convolutional networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    month = "November",
    pages = "197--201",
    year = "2018",
    keywords = "Audio tagging, DenseNet, Mixup, Multi-head softmax, Batch-wise loss masking",
    abstract = "In this paper, we describe the techniques and models applied to our submission for DCASE 2018 task 2: General-purpose audio tagging of Freesound content with AudioSet labels. We mainly focus on how to train deep learning models efficiently against strong augmentation and label noise. First, we conducted a single-block DenseNet architecture and multi-head softmax classifier for efficient learning with mixup augmentation. For the label noise, we applied the batch-wise loss masking to eliminate the loss of outliers in a mini-batch. We also tried an ensemble of various models, trained by using different sampling rate or audio representation."
}

@techreport{Akiyama2019,
    Author = "Akiyama, Osamu and Sato, Junya",
    institution = "DCASE2019 Challenge",
    title = "MULTITASK LEARNING AND SEMI-SUPERVISED LEARNING WITH NOISY DATA FOR AUDIO TAGGING",
    abstract = {This paper describes our submission to the DCASE 2019 challenge Task 2 "Audio tagging with noisy labels and minimal supervision" [1]. This task is a multi-label audio classification with 80 classes. The training data is composed of a small amount of reliably labeled data (curated data) and a larger amount of data with unreliable labels (noisy data). Additionally, there is a difference between data distribution between curated data and noisy data. To tackle this difficulty, we propose three strategies. The first is multitask learning using noisy data. The second is semi-supervised learning (SSL) using input data with a different distribution from labeled input data. The third is an ensemble method that averages models learned with different time windows. By using these methods, we achieved a score of 0.750 with label-weighted label-ranking average precision (lwlrap), which is in the top 1\% on the public leaderboard (LB).},
    month = "June",
    year = "2019"
}

@techreport{Chen2019,
    Author = "Chen, Hangting and Liu, Zuozhen and Liu, Zongming and Zhang, Pengyuan and Yan, Yonghong",
    institution = "DCASE2019 Challenge",
    title = "Integrating the Data Augmentation Scheme with Various Classifiers for Acoustic Scene Modeling",
    abstract = "This technical report describes the IOA team’s submission for TASK1A of DCASE2019 challenge. Our acoustic scene classification (ASC) system adopts a data augmentation scheme employing generative adversary networks. Two major classifiers, 1D deep convolutional neural network integrated with scalogram features and 2D fully convolutional neural network integrated with Mel filter bank features, are deployed in the scheme. Other approaches, such as adversary city adaptation, temporal module based on discrete cosine transform and hybrid architectures, have been developed for further fusion. The results of our experiments indicates that the final fusion systems A-D could achieve the accuracy above 85\% on the officially provided fold 1 evaluation dataset.",
    month = "June",
    year = "2019"
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@article{pons2018training,
    author = {Pons, J. and Serr\`a, J. and Serra, X.},
    title = {Training neural audio classifiers with few data},
    journal = {ArXiv},
    volume = {1810.10274},
    year = 2018,
    }
    
@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@inproceedings{girshick2014rich,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation},
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={580--587},
  year={2014}
}

@inproceedings{Lasseck2018,
    author = "Lasseck, Mario",
    title = "Acoustic bird detection with deep convolutional neural networks",
    booktitle = "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2018 Workshop (DCASE2018)",
    month = "November",
    pages = "143--147",
    year = "2018",
    keywords = "Bird Detection, Deep Learning, Deep Convolutional Neural Networks, Data Augmentation",
    abstract = "This paper presents deep learning techniques for acoustic bird detection. Deep Convolutional Neural Networks (DCNNs), originally designed for image classification, are adapted and fine-tuned to detect the presence of birds in audio recordings. Various data augmentation techniques are applied to increase model performance and improve generalization to unknown recording conditions and new habitats. The proposed approach is evaluated on the dataset of the Bird Audio Detection task which is part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) 2018. It surpasses previous state-of-the-art achieving an area under the curve (AUC) above 95 \\% on the public challenge leaderboard."
}

@article{zhang2019attention,
  title={Attention based Convolutional Recurrent Neural Network for Environmental Sound Classification},
  author={Zhang, Zhichao and Xu, Shugong and Qiao, Tianhao and Zhang, Shunqing and Cao, Shan},
  journal={arXiv preprint arXiv:1907.02230},
  year={2019}
}

@inproceedings{salamon2015feature,
  title={Feature learning with deep scattering for urban sound analysis},
  author={Salamon, Justin and Bello, Juan Pablo},
  booktitle={2015 23rd European Signal Processing Conference (EUSIPCO)},
  pages={724--728},
  year={2015},
  organization={IEEE}
}

@article{wyse2017audio,
  title={Audio spectrogram representations for processing with convolutional neural networks},
  author={Wyse, Lonce},
  journal={arXiv preprint arXiv:1706.09559},
  year={2017}
}

@article{abdoli2019end,
  title={End-to-End Environmental Sound Classification using a 1D Convolutional Neural Network},
  author={Abdoli, Sajjad and Cardinal, Patrick and Koerich, Alessandro Lameiras},
  journal={Expert Systems with Applications},
  year={2019},
  publisher={Elsevier}
}

@inproceedings{fonseca2019learning,
  title={Learning sound event classifiers from web audio with noisy labels},
  author={Fonseca, Eduardo and Plakal, Manoj and Ellis, Daniel PW and Font, Frederic and Favory, Xavier and Serra, Xavier},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={21--25},
  year={2019},
  organization={IEEE}
}

@inproceedings{aytar2016soundnet,
  title={Soundnet: Learning sound representations from unlabeled video},
  author={Aytar, Yusuf and Vondrick, Carl and Torralba, Antonio},
  booktitle={Advances in neural information processing systems},
  pages={892--900},
  year={2016}
}

@inproceedings{cramer2019look,
  title={Look, Listen, and Learn More: Design Choices for Deep Audio Embeddings},
  author={Cramer, Jason and Wu, Ho-Hsiang and Salamon, Justin and Bello, Juan Pablo},
  booktitle={ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3852--3856},
  year={2019},
  organization={IEEE}
}

@incollection{prechelt1998early,
  title={Early stopping-but when?},
  author={Prechelt, Lutz},
  booktitle={Neural Networks: Tricks of the trade},
  pages={55--69},
  year={1998},
  publisher={Springer}
}

@techreport{Ng2019,
    Author = "Ng, Linus and Ooi, Kenneth",
    abstract = "Identifying urban noises and sounds is a challenging but important problem in the field of machine listening [1]. It enables and provides a realistic use case for detecting noises in an urbanised city - from noise complaints to detecting sounds or unusual noises that may indicate possible emergencies. The Urban Sound Tagging challenge as part of the DCASE 2019 challenge [2] [3] addresses the problem statement of urban noise control [1]. For this challenge, we are tasked to build a audio classifier to predict whether each of 23 sources of noise pollution is present or absent in a 10-second scene, as recorded by an acoustic sensor network. In this technical report, we will examine in some detail the performance of the audio classification models trained with different open external datasets.",
    month = "September",
    year = "2019",
    title = "Urban Sound Tagging {DCASE} 2019 Chalelnge Task 5",
    institution = "DCASE2019 Challenge"
}

@techreport{Bai2019,
    Author = "Bai, Jisheng and Chen, Chen",
    abstract = "This paper presents a multi-feature fusion system for the DCASE 2019 Task5 Urban Sound Tagging(UST). It focus on predicting whether each of 23 sources of noise pollution is pre-sent or absent in a 10-second scene [1]. There are coarse-level and fine -level taxonomies to train model. We mainly focus on coarse-level and use best coarse-level model architecture to train fine-level model. Various features are extracted from original urban sound and Convolutional Neural Networks(CNNs) are applied in this system. Log-Mel, harmonic, short time Fourier transform (STFT) and Mel Frequency Cepstral Coefficents (MFCC) spectrograms are fed into a 5-layer or 9-layer CNN, and a type of gated activation [2] is also used in CNN. Different feature is adapted for different urban sound classification ac-cording to the results of our experiment. We get at least 0.14 macro-auprc score improvement compared to baseline system on coarse-level. Finally, we make a fusion of some models and evaluate on evaluation dataset.",
    month = "September",
    year = "2019",
    title = "Urban Sound Tagging with Multi-Feature Fusion System",
    institution = "DCASE2019 Challenge"
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@inproceedings{xie2017aggregated,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@article{tan2019efficientnet,
  title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  author={Tan, Mingxing and Le, Quoc V},
  journal={arXiv preprint arXiv:1905.11946},
  year={2019}
}